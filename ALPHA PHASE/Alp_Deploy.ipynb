{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb98ae4a",
   "metadata": {},
   "source": [
    "# Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddf84288",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 14:38:50.287481: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-20 14:38:50.287507: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-20 14:38:50.288200: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-20 14:38:50.292043: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-20 14:38:50.752290: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/local/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# TensorFlow/Keras imports\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# For deployment\n",
    "from flask import Flask, request, jsonify\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e1d3ff",
   "metadata": {},
   "source": [
    "# Model Loading and Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44c31ce2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 25088)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               6422784   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 14)                3598      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21141070 (80.65 MB)\n",
      "Trainable params: 6426382 (24.51 MB)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom layers\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(14, activation='softmax')(x)  # Adjust the number of classes\n",
    "\n",
    "# Compile the final model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36d612b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  4\n",
      "Visible devices cannot be modified after being initialized\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check if any GPU is detected\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set TensorFlow to use only the first GPU\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        print(f\"Using GPU: {gpus[0]}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea545914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1445 images belonging to 14 classes.\n",
      "Found 420 images belonging to 14 classes.\n",
      "Epoch 1/100\n",
      "45/45 [==============================] - 9s 189ms/step - loss: 1.3816 - accuracy: 0.5570 - val_loss: 0.9784 - val_accuracy: 0.6899\n",
      "Epoch 2/100\n",
      "45/45 [==============================] - 9s 190ms/step - loss: 1.2248 - accuracy: 0.6014 - val_loss: 0.8545 - val_accuracy: 0.7236\n",
      "Epoch 3/100\n",
      "45/45 [==============================] - 9s 189ms/step - loss: 1.1338 - accuracy: 0.6433 - val_loss: 0.8291 - val_accuracy: 0.7091\n",
      "Epoch 4/100\n",
      "45/45 [==============================] - 9s 190ms/step - loss: 1.0720 - accuracy: 0.6299 - val_loss: 0.7051 - val_accuracy: 0.7957\n",
      "Epoch 5/100\n",
      "45/45 [==============================] - 9s 188ms/step - loss: 0.9870 - accuracy: 0.6879 - val_loss: 0.6874 - val_accuracy: 0.7692\n",
      "Epoch 6/100\n",
      "45/45 [==============================] - 9s 188ms/step - loss: 0.9331 - accuracy: 0.6837 - val_loss: 0.6738 - val_accuracy: 0.7500\n",
      "Epoch 7/100\n",
      "45/45 [==============================] - 8s 185ms/step - loss: 0.8710 - accuracy: 0.7013 - val_loss: 0.5998 - val_accuracy: 0.7644\n",
      "Epoch 8/100\n",
      "45/45 [==============================] - 8s 186ms/step - loss: 0.8579 - accuracy: 0.7098 - val_loss: 0.6173 - val_accuracy: 0.7620\n",
      "Epoch 9/100\n",
      "45/45 [==============================] - 8s 187ms/step - loss: 0.7952 - accuracy: 0.7254 - val_loss: 0.5652 - val_accuracy: 0.7716\n",
      "Epoch 10/100\n",
      "45/45 [==============================] - 9s 191ms/step - loss: 0.7724 - accuracy: 0.7318 - val_loss: 0.5502 - val_accuracy: 0.7716\n",
      "Epoch 11/100\n",
      "45/45 [==============================] - 9s 191ms/step - loss: 0.7462 - accuracy: 0.7558 - val_loss: 0.5336 - val_accuracy: 0.7957\n",
      "Epoch 12/100\n",
      "45/45 [==============================] - 9s 190ms/step - loss: 0.7670 - accuracy: 0.7346 - val_loss: 0.5139 - val_accuracy: 0.8125\n",
      "Epoch 13/100\n",
      "45/45 [==============================] - 9s 191ms/step - loss: 0.7201 - accuracy: 0.7573 - val_loss: 0.4757 - val_accuracy: 0.8029\n",
      "Epoch 14/100\n",
      "45/45 [==============================] - 9s 190ms/step - loss: 0.6763 - accuracy: 0.7693 - val_loss: 0.4810 - val_accuracy: 0.8125\n",
      "Epoch 15/100\n",
      "45/45 [==============================] - 8s 186ms/step - loss: 0.6737 - accuracy: 0.7636 - val_loss: 0.4769 - val_accuracy: 0.8221\n",
      "Epoch 16/100\n",
      "45/45 [==============================] - 8s 186ms/step - loss: 0.6608 - accuracy: 0.7856 - val_loss: 0.4603 - val_accuracy: 0.8149\n",
      "Epoch 17/100\n",
      "45/45 [==============================] - 8s 187ms/step - loss: 0.6318 - accuracy: 0.7707 - val_loss: 0.4399 - val_accuracy: 0.8221\n",
      "Epoch 18/100\n",
      "45/45 [==============================] - 8s 187ms/step - loss: 0.6384 - accuracy: 0.7686 - val_loss: 0.4330 - val_accuracy: 0.8197\n",
      "Epoch 19/100\n",
      "45/45 [==============================] - 9s 190ms/step - loss: 0.6235 - accuracy: 0.7898 - val_loss: 0.4453 - val_accuracy: 0.8173\n",
      "Epoch 20/100\n",
      "45/45 [==============================] - 9s 191ms/step - loss: 0.5748 - accuracy: 0.7919 - val_loss: 0.3994 - val_accuracy: 0.8510\n",
      "Epoch 21/100\n",
      "45/45 [==============================] - 9s 188ms/step - loss: 0.5651 - accuracy: 0.7983 - val_loss: 0.4055 - val_accuracy: 0.8413\n",
      "Epoch 22/100\n",
      "45/45 [==============================] - 8s 187ms/step - loss: 0.5813 - accuracy: 0.7905 - val_loss: 0.4037 - val_accuracy: 0.8245\n",
      "Epoch 23/100\n",
      "45/45 [==============================] - 9s 190ms/step - loss: 0.5516 - accuracy: 0.8025 - val_loss: 0.3910 - val_accuracy: 0.8389\n",
      "Epoch 24/100\n",
      "45/45 [==============================] - 9s 190ms/step - loss: 0.5683 - accuracy: 0.7962 - val_loss: 0.3928 - val_accuracy: 0.8582\n",
      "Epoch 25/100\n",
      "45/45 [==============================] - 8s 186ms/step - loss: 0.5321 - accuracy: 0.8082 - val_loss: 0.3901 - val_accuracy: 0.8341\n",
      "Epoch 26/100\n",
      "45/45 [==============================] - 8s 187ms/step - loss: 0.5301 - accuracy: 0.8018 - val_loss: 0.3973 - val_accuracy: 0.8317\n",
      "Epoch 27/100\n",
      "45/45 [==============================] - 9s 188ms/step - loss: 0.5512 - accuracy: 0.7955 - val_loss: 0.4057 - val_accuracy: 0.8197\n",
      "Epoch 28/100\n",
      "45/45 [==============================] - 9s 189ms/step - loss: 0.5400 - accuracy: 0.8025 - val_loss: 0.3912 - val_accuracy: 0.8149\n",
      "Epoch 29/100\n",
      "45/45 [==============================] - 9s 189ms/step - loss: 0.5369 - accuracy: 0.8096 - val_loss: 0.3677 - val_accuracy: 0.8462\n",
      "Epoch 30/100\n",
      "45/45 [==============================] - 9s 189ms/step - loss: 0.5123 - accuracy: 0.8195 - val_loss: 0.3965 - val_accuracy: 0.8317\n",
      "Epoch 31/100\n",
      "45/45 [==============================] - 9s 190ms/step - loss: 0.5111 - accuracy: 0.8125 - val_loss: 0.3879 - val_accuracy: 0.8317\n",
      "Epoch 32/100\n",
      "45/45 [==============================] - 9s 188ms/step - loss: 0.4968 - accuracy: 0.8132 - val_loss: 0.3577 - val_accuracy: 0.8510\n",
      "Epoch 33/100\n",
      "45/45 [==============================] - 8s 186ms/step - loss: 0.4872 - accuracy: 0.8238 - val_loss: 0.3619 - val_accuracy: 0.8510\n",
      "Epoch 34/100\n",
      "45/45 [==============================] - 8s 186ms/step - loss: 0.5210 - accuracy: 0.8139 - val_loss: 0.3680 - val_accuracy: 0.8317\n",
      "Epoch 35/100\n",
      "45/45 [==============================] - 9s 189ms/step - loss: 0.4689 - accuracy: 0.8280 - val_loss: 0.3758 - val_accuracy: 0.8269\n",
      "Epoch 36/100\n",
      "45/45 [==============================] - 9s 189ms/step - loss: 0.4603 - accuracy: 0.8266 - val_loss: 0.3522 - val_accuracy: 0.8389\n",
      "Epoch 37/100\n",
      "45/45 [==============================] - 8s 187ms/step - loss: 0.4572 - accuracy: 0.8259 - val_loss: 0.3583 - val_accuracy: 0.8462\n",
      "Epoch 38/100\n",
      "45/45 [==============================] - 9s 192ms/step - loss: 0.4549 - accuracy: 0.8299 - val_loss: 0.3390 - val_accuracy: 0.8678\n",
      "Epoch 39/100\n",
      "45/45 [==============================] - 8s 186ms/step - loss: 0.4369 - accuracy: 0.8309 - val_loss: 0.3346 - val_accuracy: 0.8438\n",
      "Epoch 40/100\n",
      "45/45 [==============================] - 9s 190ms/step - loss: 0.4463 - accuracy: 0.8231 - val_loss: 0.3421 - val_accuracy: 0.8389\n",
      "Epoch 41/100\n",
      "45/45 [==============================] - 8s 185ms/step - loss: 0.4484 - accuracy: 0.8372 - val_loss: 0.3510 - val_accuracy: 0.8389\n",
      "Epoch 42/100\n",
      "45/45 [==============================] - 9s 189ms/step - loss: 0.4515 - accuracy: 0.8365 - val_loss: 0.3462 - val_accuracy: 0.8438\n",
      "Epoch 43/100\n",
      "45/45 [==============================] - 8s 185ms/step - loss: 0.4250 - accuracy: 0.8401 - val_loss: 0.3352 - val_accuracy: 0.8534\n",
      "Epoch 44/100\n",
      "45/45 [==============================] - 8s 186ms/step - loss: 0.4440 - accuracy: 0.8280 - val_loss: 0.3307 - val_accuracy: 0.8486\n",
      "Epoch 45/100\n",
      "45/45 [==============================] - 8s 187ms/step - loss: 0.3932 - accuracy: 0.8507 - val_loss: 0.3164 - val_accuracy: 0.8630\n",
      "Epoch 46/100\n",
      "45/45 [==============================] - 9s 190ms/step - loss: 0.4336 - accuracy: 0.8301 - val_loss: 0.3389 - val_accuracy: 0.8413\n",
      "Epoch 47/100\n",
      "45/45 [==============================] - 8s 185ms/step - loss: 0.4048 - accuracy: 0.8478 - val_loss: 0.3369 - val_accuracy: 0.8293\n",
      "Epoch 48/100\n",
      "45/45 [==============================] - 8s 186ms/step - loss: 0.4263 - accuracy: 0.8450 - val_loss: 0.3202 - val_accuracy: 0.8558\n",
      "Epoch 49/100\n",
      "45/45 [==============================] - 8s 185ms/step - loss: 0.4133 - accuracy: 0.8436 - val_loss: 0.3149 - val_accuracy: 0.8510\n",
      "Epoch 50/100\n",
      "45/45 [==============================] - 8s 184ms/step - loss: 0.4225 - accuracy: 0.8358 - val_loss: 0.3217 - val_accuracy: 0.8702\n",
      "Epoch 51/100\n",
      "45/45 [==============================] - 8s 185ms/step - loss: 0.4053 - accuracy: 0.8464 - val_loss: 0.3091 - val_accuracy: 0.8510\n",
      "Epoch 52/100\n",
      "45/45 [==============================] - 8s 185ms/step - loss: 0.3997 - accuracy: 0.8436 - val_loss: 0.3371 - val_accuracy: 0.8341\n",
      "Epoch 53/100\n",
      "45/45 [==============================] - 9s 195ms/step - loss: 0.4075 - accuracy: 0.8457 - val_loss: 0.3492 - val_accuracy: 0.8438\n",
      "Epoch 54/100\n",
      "45/45 [==============================] - 8s 186ms/step - loss: 0.3930 - accuracy: 0.8450 - val_loss: 0.3339 - val_accuracy: 0.8462\n",
      "Epoch 55/100\n",
      "45/45 [==============================] - 8s 187ms/step - loss: 0.3975 - accuracy: 0.8450 - val_loss: 0.3187 - val_accuracy: 0.8654\n",
      "Epoch 56/100\n",
      "45/45 [==============================] - 8s 188ms/step - loss: 0.3961 - accuracy: 0.8386 - val_loss: 0.3367 - val_accuracy: 0.8413\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 9s 188ms/step - loss: 0.3885 - accuracy: 0.8542 - val_loss: 0.3338 - val_accuracy: 0.8534\n",
      "Epoch 58/100\n",
      "45/45 [==============================] - 9s 190ms/step - loss: 0.3684 - accuracy: 0.8576 - val_loss: 0.3095 - val_accuracy: 0.8822\n",
      "Epoch 59/100\n",
      "45/45 [==============================] - 8s 184ms/step - loss: 0.3910 - accuracy: 0.8542 - val_loss: 0.3274 - val_accuracy: 0.8486\n",
      "Epoch 60/100\n",
      "45/45 [==============================] - 8s 187ms/step - loss: 0.3796 - accuracy: 0.8599 - val_loss: 0.3217 - val_accuracy: 0.8486\n",
      "Epoch 61/100\n",
      "45/45 [==============================] - 9s 188ms/step - loss: 0.3535 - accuracy: 0.8620 - val_loss: 0.3375 - val_accuracy: 0.8582\n",
      "Epoch 62/100\n",
      "45/45 [==============================] - 8s 187ms/step - loss: 0.3912 - accuracy: 0.8613 - val_loss: 0.3210 - val_accuracy: 0.8558\n",
      "Epoch 63/100\n",
      "45/45 [==============================] - 9s 191ms/step - loss: 0.3716 - accuracy: 0.8549 - val_loss: 0.3099 - val_accuracy: 0.8726\n",
      "Epoch 64/100\n",
      "45/45 [==============================] - 8s 184ms/step - loss: 0.3674 - accuracy: 0.8570 - val_loss: 0.3164 - val_accuracy: 0.8582\n",
      "Epoch 65/100\n",
      "45/45 [==============================] - 8s 187ms/step - loss: 0.3509 - accuracy: 0.8684 - val_loss: 0.3463 - val_accuracy: 0.8438\n",
      "Epoch 66/100\n",
      "45/45 [==============================] - 8s 187ms/step - loss: 0.3629 - accuracy: 0.8627 - val_loss: 0.3088 - val_accuracy: 0.8654\n",
      "Epoch 67/100\n",
      "45/45 [==============================] - 9s 189ms/step - loss: 0.3586 - accuracy: 0.8655 - val_loss: 0.3169 - val_accuracy: 0.8702\n",
      "Epoch 68/100\n",
      "45/45 [==============================] - 9s 188ms/step - loss: 0.3698 - accuracy: 0.8613 - val_loss: 0.2955 - val_accuracy: 0.8846\n",
      "Epoch 69/100\n",
      "45/45 [==============================] - 8s 185ms/step - loss: 0.3783 - accuracy: 0.8493 - val_loss: 0.3079 - val_accuracy: 0.8389\n",
      "Epoch 70/100\n",
      "45/45 [==============================] - 9s 189ms/step - loss: 0.3482 - accuracy: 0.8606 - val_loss: 0.3226 - val_accuracy: 0.8462\n",
      "Epoch 71/100\n",
      "45/45 [==============================] - 9s 191ms/step - loss: 0.3385 - accuracy: 0.8641 - val_loss: 0.3231 - val_accuracy: 0.8558\n",
      "Epoch 72/100\n",
      "45/45 [==============================] - 8s 187ms/step - loss: 0.3627 - accuracy: 0.8627 - val_loss: 0.3347 - val_accuracy: 0.8365\n",
      "Epoch 73/100\n",
      "45/45 [==============================] - 9s 189ms/step - loss: 0.3550 - accuracy: 0.8613 - val_loss: 0.3113 - val_accuracy: 0.8582\n",
      "Epoch 74/100\n",
      "45/45 [==============================] - 9s 189ms/step - loss: 0.3391 - accuracy: 0.8698 - val_loss: 0.3057 - val_accuracy: 0.8582\n",
      "Epoch 75/100\n",
      "45/45 [==============================] - 9s 189ms/step - loss: 0.3490 - accuracy: 0.8599 - val_loss: 0.3269 - val_accuracy: 0.8462\n",
      "Epoch 76/100\n",
      "45/45 [==============================] - 8s 184ms/step - loss: 0.3595 - accuracy: 0.8641 - val_loss: 0.2976 - val_accuracy: 0.8534\n",
      "Epoch 77/100\n",
      "45/45 [==============================] - 9s 189ms/step - loss: 0.3237 - accuracy: 0.8740 - val_loss: 0.3206 - val_accuracy: 0.8438\n",
      "Epoch 78/100\n",
      "45/45 [==============================] - 8s 185ms/step - loss: 0.3190 - accuracy: 0.8776 - val_loss: 0.2990 - val_accuracy: 0.8510\n",
      "Epoch 79/100\n",
      "45/45 [==============================] - 9s 189ms/step - loss: 0.3509 - accuracy: 0.8662 - val_loss: 0.3150 - val_accuracy: 0.8293\n",
      "Epoch 80/100\n",
      "45/45 [==============================] - 8s 186ms/step - loss: 0.3367 - accuracy: 0.8606 - val_loss: 0.2996 - val_accuracy: 0.8582\n",
      "Epoch 81/100\n",
      "45/45 [==============================] - 9s 189ms/step - loss: 0.3398 - accuracy: 0.8556 - val_loss: 0.2913 - val_accuracy: 0.8750\n",
      "Epoch 82/100\n",
      "45/45 [==============================] - 8s 186ms/step - loss: 0.3315 - accuracy: 0.8712 - val_loss: 0.2934 - val_accuracy: 0.8678\n",
      "Epoch 83/100\n",
      "45/45 [==============================] - 8s 187ms/step - loss: 0.3229 - accuracy: 0.8762 - val_loss: 0.2857 - val_accuracy: 0.8630\n",
      "Epoch 84/100\n",
      "45/45 [==============================] - 9s 189ms/step - loss: 0.3270 - accuracy: 0.8669 - val_loss: 0.2935 - val_accuracy: 0.8774\n",
      "Epoch 85/100\n",
      "45/45 [==============================] - 9s 189ms/step - loss: 0.3001 - accuracy: 0.8783 - val_loss: 0.2965 - val_accuracy: 0.8678\n",
      "Epoch 86/100\n",
      "45/45 [==============================] - 9s 191ms/step - loss: 0.3369 - accuracy: 0.8641 - val_loss: 0.2935 - val_accuracy: 0.8750\n",
      "Epoch 87/100\n",
      "45/45 [==============================] - 8s 186ms/step - loss: 0.3185 - accuracy: 0.8797 - val_loss: 0.2821 - val_accuracy: 0.8606\n",
      "Epoch 88/100\n",
      "45/45 [==============================] - 9s 189ms/step - loss: 0.3184 - accuracy: 0.8790 - val_loss: 0.2881 - val_accuracy: 0.8582\n",
      "Epoch 89/100\n",
      "45/45 [==============================] - 8s 187ms/step - loss: 0.3095 - accuracy: 0.8839 - val_loss: 0.2773 - val_accuracy: 0.8966\n",
      "Epoch 90/100\n",
      "45/45 [==============================] - 9s 191ms/step - loss: 0.3160 - accuracy: 0.8785 - val_loss: 0.2732 - val_accuracy: 0.8750\n",
      "Epoch 91/100\n",
      "45/45 [==============================] - 8s 185ms/step - loss: 0.3022 - accuracy: 0.8854 - val_loss: 0.2715 - val_accuracy: 0.8678\n",
      "Epoch 92/100\n",
      "45/45 [==============================] - 9s 189ms/step - loss: 0.3077 - accuracy: 0.8818 - val_loss: 0.2731 - val_accuracy: 0.8870\n",
      "Epoch 93/100\n",
      "45/45 [==============================] - 9s 191ms/step - loss: 0.3099 - accuracy: 0.8776 - val_loss: 0.2703 - val_accuracy: 0.8750\n",
      "Epoch 94/100\n",
      "45/45 [==============================] - 9s 188ms/step - loss: 0.2884 - accuracy: 0.8981 - val_loss: 0.2835 - val_accuracy: 0.8654\n",
      "Epoch 95/100\n",
      "45/45 [==============================] - 9s 191ms/step - loss: 0.3195 - accuracy: 0.8811 - val_loss: 0.2961 - val_accuracy: 0.8438\n",
      "Epoch 96/100\n",
      "45/45 [==============================] - 9s 189ms/step - loss: 0.2957 - accuracy: 0.8818 - val_loss: 0.2742 - val_accuracy: 0.8678\n",
      "Epoch 97/100\n",
      "45/45 [==============================] - 9s 188ms/step - loss: 0.3010 - accuracy: 0.8889 - val_loss: 0.2540 - val_accuracy: 0.8846\n",
      "Epoch 98/100\n",
      "45/45 [==============================] - 9s 191ms/step - loss: 0.2950 - accuracy: 0.8868 - val_loss: 0.2761 - val_accuracy: 0.8774\n",
      "Epoch 99/100\n",
      "45/45 [==============================] - 8s 187ms/step - loss: 0.2997 - accuracy: 0.8839 - val_loss: 0.2714 - val_accuracy: 0.8702\n",
      "Epoch 100/100\n",
      "45/45 [==============================] - 8s 185ms/step - loss: 0.3171 - accuracy: 0.8726 - val_loss: 0.2734 - val_accuracy: 0.8750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exh4748/.local/lib/python3.9/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Paths to data directories\n",
    "train_dir = '/home/exh4748/ProjectTortoise/split_data/train'\n",
    "val_dir = '/home/exh4748/ProjectTortoise/split_data/val'\n",
    "\n",
    "# Data generators\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "# Data loaders\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir, target_size=(224, 224), batch_size=32, class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir, target_size=(224, 224), batch_size=32, class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=100,  # Adjust as needed\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_steps=val_generator.samples // val_generator.batch_size\n",
    ")\n",
    "\n",
    "# Save the model for later use\n",
    "model.save('facial_recognition_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e425a960",
   "metadata": {},
   "source": [
    "## Testing it on Efaz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1abb216c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 427 images belonging to 14 classes.\n",
      "13/13 [==============================] - 0s 32ms/step - loss: 0.2828 - accuracy: 0.8558\n",
      "Test Loss: 0.2828\n",
      "Test Accuracy: 85.58%\n",
      "14/14 [==============================] - 1s 34ms/step\n",
      "Predicted Labels: ['ahmed', 'ahmed', 'ahmed', 'ahmed', 'ahmed']\n"
     ]
    }
   ],
   "source": [
    "# Test data directory\n",
    "test_dir = '/home/exh4748/ProjectTortoise/split_data/test'\n",
    "\n",
    "# Test data generator\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "# Test generator\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(test_generator, steps=test_generator.samples // test_generator.batch_size)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Predict all test samples\n",
    "predictions = model.predict(test_generator)\n",
    "predicted_classes = predictions.argmax(axis=1)\n",
    "\n",
    "# Map indices to labels\n",
    "class_labels = list(test_generator.class_indices.keys())\n",
    "predicted_labels = [class_labels[idx] for idx in predicted_classes]\n",
    "\n",
    "# Print sample predictions\n",
    "print(\"Predicted Labels:\", predicted_labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8eff1d",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "352f5af0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 92ms/step\n",
      "Image: /home/exh4748/ProjectTortoise/split_data/test/efaz_augmented/20231112_204757_0_1951.jpg\n",
      "Predicted Class: efaz_augmented, Confidence: 0.70\n",
      "--------------------------------------------------\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Image: /home/exh4748/ProjectTortoise/split_data/test/giannis_augmented/w640xh480_GettyImages-1238985680_0_2952.jpg\n",
      "Predicted Class: giannis_augmented, Confidence: 1.00\n",
      "--------------------------------------------------\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Image: /home/exh4748/ProjectTortoise/split_data/test/ahmed/ahmed_0_3633.jpeg\n",
      "Predicted Class: ahmed, Confidence: 0.85\n",
      "--------------------------------------------------\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Image: /home/exh4748/ProjectTortoise/split_data/test/efaz/efaz_0_7093.jpeg\n",
      "Predicted Class: efaz, Confidence: 0.79\n",
      "--------------------------------------------------\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Image: /home/exh4748/ProjectTortoise/split_data/test/jokic_augmented/jokic_augmented_0_1529.jpeg\n",
      "Predicted Class: jokic_augmented, Confidence: 1.00\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Load the saved model\n",
    "model = load_model('facial_recognition_model.h5')\n",
    "\n",
    "# Function to preprocess images\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(224, 224))  # Adjust to model's input size\n",
    "    img_array = img_to_array(img) / 255.0  # Normalize the image\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    return img_array\n",
    "\n",
    "# Function for prediction\n",
    "def predict(image_path):\n",
    "    processed_image = preprocess_image(image_path)\n",
    "    predictions = model.predict(processed_image)\n",
    "    class_index = np.argmax(predictions)\n",
    "    confidence = np.max(predictions)\n",
    "    return class_index, confidence\n",
    "\n",
    "# Randomly pick and predict images from the test directory\n",
    "def predict_random_images(test_dir, num_images=5):\n",
    "    # Collect all image paths from test directory\n",
    "    all_images = []\n",
    "    for root, _, files in os.walk(test_dir):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('png', 'jpg', 'jpeg')):\n",
    "                all_images.append(os.path.join(root, file))\n",
    "    \n",
    "    # Randomly select images to predict\n",
    "    selected_images = random.sample(all_images, min(num_images, len(all_images)))\n",
    "\n",
    "    # Class labels (adjust this based on your dataset)\n",
    "    class_labels = list(train_generator.class_indices.keys())  # Use the same class indices as your training generator\n",
    "\n",
    "    # Predict and display results\n",
    "    for image_path in selected_images:\n",
    "        class_index, confidence = predict(image_path)\n",
    "        predicted_label = class_labels[class_index]\n",
    "        print(f\"Image: {image_path}\")\n",
    "        print(f\"Predicted Class: {predicted_label}, Confidence: {confidence:.2f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Path to test directory\n",
    "test_dir = '/home/exh4748/ProjectTortoise/split_data/test'\n",
    "\n",
    "# Predict random images\n",
    "predict_random_images(test_dir, num_images=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ca2a3e",
   "metadata": {},
   "source": [
    "# Flask Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226c8a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import numpy as np\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('facial_recognition_model.h5')\n",
    "\n",
    "# Initialize the Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(224, 224))  # Match the input size of the model\n",
    "    img_array = img_to_array(img) / 255.0  # Normalize the image\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    return img_array\n",
    "\n",
    "# Prediction route\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    if 'image' not in request.files:\n",
    "        return jsonify({'error': 'No image file provided'}), 400\n",
    "\n",
    "    # Save the uploaded image\n",
    "    image_file = request.files['image']\n",
    "    image_path = './temp.jpg'\n",
    "    image_file.save(image_path)\n",
    "\n",
    "    # Preprocess the image\n",
    "    processed_image = preprocess_image(image_path)\n",
    "\n",
    "    # Make prediction\n",
    "    predictions = model.predict(processed_image)\n",
    "    class_index = np.argmax(predictions)\n",
    "    confidence = np.max(predictions)\n",
    "\n",
    "    # Map the class index to the label\n",
    "    class_labels = list(train_generator.class_indices.keys())  # Adjust based on your class labels\n",
    "    predicted_label = class_labels[class_index]\n",
    "\n",
    "    return jsonify({\n",
    "        'predicted_label': predicted_label,\n",
    "        'confidence': float(confidence)\n",
    "    })\n",
    "\n",
    "# Run the app\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True, port=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0013c702",
   "metadata": {},
   "source": [
    "# Testing API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8a472e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'http://127.0.0.1:5000/predict'\n",
    "# Replace with image_path\n",
    "files = {'image': open(image_path, 'rb')}\n",
    "response = requests.post(url, files=files)\n",
    "\n",
    "print(response.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
